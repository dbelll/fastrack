Optimize:
	reduce number of arguments, used constant memory instead
	Use more registers instead of shared memory?
	Compact the memory for global weights
	

Organize
Today's Goals

	Vary episode length by agent
	Experiment with frequency of replication

Questions:
	Will modifying poor agents' parameters help or hurt
	What is the best number of parallel opponents?

To do list:
	Add varying training_turns by agent
	Finish varying training_pieces by agent and test the results
	Add ability to load all initial agents from file
	
Fixes:
	Fix problem with num_pieces of 1 - must check for terminal condition after the first move.

	Add exploration
		- add noise to val_for_state
		- add epsilon
		- modify initial weights to encourage exploration
		
	Profile kernels and optimize
		choose_move is the bottleneck, has incoherent global memory reads
		<done> put *dc_moves into a texture?
	
	Re-organize global memory layout of weights to match shared memory layout
	
	Use twice as many threads in the learn kernel to speed up wgt copying.  The threads beyond
	dc_board_size then do nothing during learning.
	
	Create fixed baseline that shows good learning
	
	Review code for memory leaks oh host and device

	build a program to test agents on set board positions
	
	put dc_opgrid back to constant memory
	
	Use the macros to get values from AGENT data
		
	Reduce the number of arguments by storing the pointer to AGENT *agGPU in constant memory and
	calculating the other pointers from it.
	
	Compete against best agents from previous _2_ rounds
	
	Modify the parameters for worst agents


Todo's

Add ability to have varying num_hidden by agent



Processing

	init agents
	
	main loop
	
		learning session
			determine parameters for this learning session
			loop through all opponents
				learn against opponent for one episode
			next opponent	
		
		evaluate agents
			report stats
			replicate/vary
			
	repeat main loop for total session count


GPU Code
	check
	optimize
